{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alert-roommate",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N°1\n",
    "\n",
    "## Ejercicio 1: Clasificador de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-vegetation",
   "metadata": {},
   "source": [
    "### 1. Introducción\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En este ejercicio se propuso entrenar un modelo que clasifique textos en 20 categorías, provistas por la librería \"sklearn\". Para importar los datos se usa la función \"fetch_20newsgroups\" con parámetros train y test, dependiendo del caso, como se puede ver en el siguiente bloque de código. \n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Los archivos \"train_data.txt\" y \"test_data.txt\" son los datos de \"fetch_20newsgroups\" ya lematizados con el WordNetLemmatizer del Natural Language Toolkit (nltk). Cada una de las líneas de cada uno de los archivos corresponde a la versión lematizada de cada uno de los textos, con lo cual se los puede separar con el método \"splitLines\" de la clase string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "light-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove = ('headers', 'footers'))\n",
    "targets_train = twenty_train['target']\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers'))\n",
    "targets_test = twenty_test['target']\n",
    "\n",
    "with open('Lemmatized/train_data.txt', 'rt') as file: train_data = file.read().splitlines()\n",
    "with open('Lemmatized/test_data.txt', 'rt') as file: test_data = file.read().splitlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-canadian",
   "metadata": {},
   "source": [
    "### 2. Implementación\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Luego se define la clase \"NewsClassifier\", cuyo constructor recibe los parámetros que serán provistos al \"TfidfVectorizer\". Esta estructura se implementó para poder tener un modelo dinámico con hiperparámetros fácilmente modificables. Entre éstos se encontrarán: \n",
    "</div>\n",
    "\n",
    "- $\\textit{max_idf}$: Máxima frecuencia entre documentos permitida;\n",
    "- $\\textit{min_idf}$: Minima frecuencia entre documentos permitida;\n",
    "- $\\textit{sublinear_tf}$: Suavizado para grandes frecuencias de términos;\n",
    "- $\\textit{use_idf/smooth_idf}$: Uso de idf y suavizado para evitar divisiones por cero;\n",
    "- $\\textit{norm}$: Normalización (norma 1 o norma 2).\n",
    "\n",
    "#### 2.1. Entrenamiento\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En primer lugar se le aplica la vertorización \"TF-IDF\" a los datos de entrenamiento, usando los parámetros pasados en el constructor. Esto nos dará una matriz esparza con puntajes: sus filas y columnas serán los textos y las palabras, respectivamente. Luego se guardan las probabilidades a priori de cada una de las clases. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-environment",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Paralelamente, se define una matriz de probabilidades de dimensión 20xN, donde \"N\" es la cantidad de palabras en el vocabulario, y se itera sobre cada una de las clases. En cada una de estas iteraciones, se suman los puntajes de cada una de las palabras en noticias de la clase por la cual se está iterando. A este resultado se le aplica un factor de $\\textit{smoothing}$, con el propósito de evitar probabilidades iguales a cero. Finalmente, se divide a este vector de puntajes por la suma de todos sus componentes, para convertirlo en probabilidades. Este resultado final se guarda en la fila correspondiente de la matriz de probabilidades. Luego de iterar por todas las clases, el modelo queda entrenado y está apto para realizar predicciones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-greek",
   "metadata": {},
   "source": [
    "#### 2.2. Predicción\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "    Para predicir, primero se tomarán los textos a clasificar y se guardarán las repeticiones de cada palabra en cada noticia en en una matriz esparza. Cabe destacar que sólo serán tenidas en cuenta las palabras con las que el modelo fue entrenado. Con el proposito de utilizar \"Naive Bayes\", se quiere aplicar la siguiente fórmula:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-alliance",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $P(x_1,x_2,...,x_N|y) = \\prod_{i=1}^N P(x_i|y)^{n_{ij}}$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-defendant",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "    Donde $x_i$ es cada una de la palabras del vocabulario y $n_{ij}$ la cantidad de repeticiones de la palabra $x_i$ en el texto \"j\". Esta probabilidad obtenida será el likelihood naive, asumiendo independencia y approach BOW. Luego, se calcula este likelihood para cada una de las clases \"y\", y a cada una se la multiplica por su probabilidad a priori obtenida durante el entrenamiento. Finalmente se dirá que el texto pertenecerá a la clase con mayor probabilidad a posteriori, o sea, $P(y|x_1,x_2,...,x_N)$, que será proporcional a $P(x_1,x_2,...,x_N|y)\\cdot P(y)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-banana",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "El único cambio que en este trabajo se le aplicó a la fórmula de Naive Bayes fue el hecho de comparar los logaritmos de $P(y|x)$. Esto se decidió hacer así para evitar tener números muy pequeños debido a los productos de las probabilidades. Por lo tanto, la fórmula de Naive Bayes se reescribe de la siguiente forma:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-truth",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $\\log(P(y|x)) = \\log(P(y)) + \\sum_{i=1}^N n_{ij}\\cdot \\log(P(x_i|y))$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-charge",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En la fórmula anterior se puede ver la necesidad del $\\textit{smoothing}$ ya que, de no haberlo, se estarían evaluando logaritmos en cero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-electronics",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En el caso particular de este trabajo, con el fin de aplicar la fórmula mencionada, se tomará a la matriz de repeticiones provista por CountVectorizer, y se la multiplicará matricialmente por la matriz de logaritmos de probabilidades, traspuesta. Este producto matricial dará como resultado otra matriz, cuyas columnas serán las distintas clases y sus filas serán los distintos textos a clasificar. En los pares fila-columna $(i-j)$ estarán los logaritmos de likelihood del texto $i$ para la clase $j$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-comparative",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Si a cada columna luego se le suma el logaritmo de la probabilidad a priori de la clase correspondiente, entonces la matriz se transformará en una matriz cuyos valores serán proporcionales a los logaritmos de las probabilidades a posteriori de cada noticia para cada clase. Finalmente, el vector de predicciones serán las columnas con probabilidad a posteriori máxima de cada fila.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupied-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tfidf = TfidfVectorizer(**kwargs, stop_words = 'english')\n",
    "\n",
    "        self.got_hist = False\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, train_set, targets):\n",
    "        \n",
    "        self.got_hist = False\n",
    "        \n",
    "        self.train_set, self.targets_train = train_set, targets\n",
    "\n",
    "        self.groups = np.unique(self.targets_train)\n",
    "        \n",
    "        self.fitted_train = self.tfidf.fit_transform(self.train_set)\n",
    "        self.priori = np.mean(self.targets_train == self.groups.reshape(-1, 1), axis = 1)\n",
    "        \n",
    "        self.probabilities = np.zeros((20, self.fitted_train.shape[1]))\n",
    "        \n",
    "        for i, group in enumerate(self.groups):\n",
    "            row = self.fitted_train[self.targets_train == group].sum(axis = 0)\n",
    "            row += row[row > 0].min()\n",
    "            self.probabilities[i] = row / row.sum()\n",
    "        \n",
    "        self.cv = CountVectorizer(stop_words = 'english', vocabulary = self.tfidf.get_feature_names())\n",
    "        \n",
    "        self.fitted = True\n",
    "            \n",
    "    def df_hist(self, **kwargs):\n",
    "        if not self.got_hist:\n",
    "            df = self.fitted_train.getnnz(axis = 0) / self.targets_train.size\n",
    "        \n",
    "            self.hist_height, self.hist_x, container = plt.hist(df * 100, bins = int(np.sqrt(df.size)), **kwargs)\n",
    "            self.hist_width = (self.hist_x[-1] - self.hist_x[0]) / (self.hist_x.size - 1)\n",
    "            self.hist_x = (self.hist_x[:-1] + self.hist_x[1:])/2\n",
    "            self.got_hist = True\n",
    "        else:\n",
    "            plt.bar(self.hist_x, self.hist_height, width = self.hist_width)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('%')\n",
    "        if 'density' in kwargs and kwargs['density']: plt.ylabel('Density')\n",
    "        else: plt.ylabel('Word count')\n",
    "            \n",
    "    def score(self, test_set, targets):\n",
    "        return np.mean(self.predict(test_set) == targets)\n",
    "    \n",
    "    def check_fitted(self):\n",
    "        if not self.fitted: raise Exception ('Cannot predict before fitting.')\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        self.check_fitted()\n",
    "        \n",
    "        fitted_test = self.cv.transform(input_data)\n",
    "        probs_test = fitted_test.dot(np.log(self.probabilities).T) + np.log(self.priori)\n",
    "        \n",
    "        return self.groups[np.argmax(probs_test, axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-message",
   "metadata": {},
   "source": [
    "### 3. Análisis Exploratorio de Datos\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Para ver como se distribuyen las frecuencias entre documentos, se entrenó al modelo con los datos de entrenamiento, y se llamó a la función \"df_hist\". Ésta plotea un histograma de las frecuencias entre documentos. En la figura se puede ver que prácticamente no hay palabras que aparezcan en más del 50% de los documentos. Este dato se utilizará como referencia para elegir el hiperparámentro max_df. Además se puede ver que la gran mayoría de las palabras aparecen en un bajo porcentaje de los documentos. Este dato se utilizará para la elección del hiperparámetro min_df.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recent-adoption",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = 1.0)                    \n",
    "classifier.fit(train_data, targets_train)\n",
    "classifier.df_hist(density = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-bunny",
   "metadata": {},
   "source": [
    "### 4. Selección de hiperparámetros\n",
    "\n",
    "#### 4.1 Validación\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Para la elección de los hiperparámetros se dividió a los datos de test en un grupo de validación y uno de testeo del modelo validado. Luego, se iteró por todas las combinaciones de hiperparámetros con el fin de encontrar la combinación que maximice la métrica utilizada en este ejercicio, que es el accuracy.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriental-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_df: 0.46\n",
      "Best smoothing: False\n",
      "Best use_idf: False\n",
      "Best subtf: True\n",
      "Best norm: l2\n",
      "Best result: 0.7997875730217737\n"
     ]
    }
   ],
   "source": [
    "max_df = np.arange(.01, .57, .05)\n",
    "smooth_idf = np.array([True, False])\n",
    "use_idf = np.array([True, False])\n",
    "sublinear_tf = np.array([True, False])\n",
    "norm = np.array(['l1', 'l2'])\n",
    "validation, test, target_v, target_t = train_test_split(test_data, targets_test, test_size = 0.5, random_state = 0)\n",
    "\n",
    "res = np.zeros((max_df.size, smooth_idf.size, use_idf.size, sublinear_tf.size, norm.size))\n",
    "\n",
    "for i, df in enumerate(max_df):\n",
    "    for j, smooth in enumerate(smooth_idf):\n",
    "        for z, idf in enumerate(use_idf):\n",
    "            for k, subtf in enumerate(sublinear_tf):\n",
    "                for l, nrm in enumerate(norm):\n",
    "                    classifier = NewsClassifier(max_df = df, smooth_idf = smooth, use_idf = idf, sublinear_tf = subtf, norm = nrm)\n",
    "                    \n",
    "                    classifier.fit(train_data, targets_train)\n",
    "                    res[i, j, z, k, l] = classifier.score(validation, target_v)\n",
    "                    \n",
    "best = res.max()\n",
    "argmax = np.argwhere(res == best)[-1]\n",
    "print(f'Best max_df: {max_df[argmax[0]]}')\n",
    "print(f'Best smoothing: {smooth_idf[argmax[1]]}')\n",
    "print(f'Best use_idf: {use_idf[argmax[2]]}')\n",
    "print(f'Best subtf: {sublinear_tf[argmax[3]]}')\n",
    "print(f'Best norm: {norm[argmax[4]]}')\n",
    "print(f'Best result: {best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-slovenia",
   "metadata": {},
   "source": [
    "#### 4.2 Testeo\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Habiendo encontrado los hiperparámetros que mejor ajustaban a los datos de validación, se prosiguió a verificar dicho resultado con los datos de testeo, con el fin de evitar sesgo. Como se puede ver, el accuracy obtenido se sigue manteniendo con lo cual se considera que los hiperparámetros previamente encontrados son efectivos para ambas divisiones de dataset de test. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "colonial-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805894848645778\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = max_df[argmax[0]], sublinear_tf = True, use_idf = False, norm = 'l2')\n",
    "classifier.fit(train_data, targets_train)\n",
    "print(classifier.score(test, target_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-integer",
   "metadata": {},
   "source": [
    "### 5. Resultado final\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Finalmente se procede a encontrar el accuracy con todo el dataset de test, que se espera que sea el promedio entre el accuracy de validación y el de testeo de hiperparámetros.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "future-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028412108337759\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = max_df[argmax[0]], sublinear_tf = True, use_idf = False)                    \n",
    "classifier.fit(train_data, targets_train)\n",
    "print(classifier.score(test_data, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-astronomy",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Como modelo alternativo, se lo puede comparar con el resultante de utilizar todos los datos ($\\textit{max_df}\\approx 0.55$). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rational-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80323951141795\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = 0.55, sublinear_tf = True, use_idf = False)                    \n",
    "classifier.fit(train_data, targets_train)\n",
    "print(classifier.score(test_data, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-fitness",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Como se puede ver, los hiperparámetros presentaban un leve sesgo, ya que al utilizar todas las palabras el accuracy del dataset del test completo aumenta un 0.4%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-cardiff",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Con este resultado, se puede concluir:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-signature",
   "metadata": {},
   "source": [
    "<center> $Accuracy = 80.32\\%$ </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
