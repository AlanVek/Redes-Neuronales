{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cathedral-approval",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N°1\n",
    "\n",
    "## Ejercicio 1: Clasificador de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-species",
   "metadata": {},
   "source": [
    "### 1. Introducción\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En este ejercicio se propuso entrenar un modelo que clasifique textos en 20 categorías, provistas por la librería \"sklearn\". Para importar los datos se usa la función \"fetch_20newsgroups\" con parámetros train y test, dependiendo del caso, como se puede ver en el siguiente bloque de código. \n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Los archivos \"train_data.txt\" y \"test_data.txt\" son los datos de \"fetch_20newsgroups\" ya lematizados con el WordNetLemmatizer del Natural Language Toolkit (nltk). Cada una de las líneas de cada uno de los archivos corresponde a la versión lematizada de cada uno de los textos, con lo cual se los puede separar con el método \"splitlines\" de la clase string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "optical-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove = ('headers', 'footers'))\n",
    "targets_train = twenty_train['target']\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset = 'test', remove = ('headers', 'footers'))\n",
    "targets_test = twenty_test['target']\n",
    "\n",
    "with open('Lemmatized/train_data.txt', 'rt') as file: train_data = file.read().splitlines()\n",
    "with open('Lemmatized/test_data.txt', 'rt') as file: test_data = file.read().splitlines() \n",
    "    \n",
    "train_data, val_data, targets_train, targets_val = train_test_split(train_data, targets_train, test_size = .2, shuffle = False, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-separation",
   "metadata": {},
   "source": [
    "### 2. Implementación\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Luego se define la clase \"NewsClassifier\", cuyo constructor recibe los parámetros que serán provistos al \"TfidfVectorizer\". Esta estructura se implementó para poder tener un modelo dinámico con hiperparámetros fácilmente modificables. Entre éstos se encontrarán: \n",
    "</div>\n",
    "\n",
    "- $\\textit{max_idf}$: Máxima frecuencia entre documentos permitida;\n",
    "- $\\textit{min_idf}$: Minima frecuencia entre documentos permitida;\n",
    "- $\\textit{sublinear_tf}$: Suavizado para grandes frecuencias de términos;\n",
    "- $\\textit{use_idf/smooth_idf}$: Uso de idf y suavizado para evitar divisiones por cero;\n",
    "- $\\textit{norm}$: Normalización (norma 1 o norma 2).\n",
    "\n",
    "#### 2.1. Entrenamiento\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En primer lugar se le aplica la vertorización \"TF-IDF\" a los datos de entrenamiento, usando los parámetros pasados en el constructor. Esto nos dará una matriz esparza con puntajes: sus filas y columnas serán los textos y las palabras, respectivamente. Luego se guardan las probabilidades a priori de cada una de las clases. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-transaction",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Paralelamente, se define una matriz de probabilidades de dimensión 20xN, donde \"N\" es la cantidad de palabras en el vocabulario, y se itera sobre cada una de las clases. En cada una de estas iteraciones, se suman los puntajes de cada una de las palabras en noticias de la clase por la cual se está iterando. A este resultado se le aplica un factor de $\\textit{smoothing}$, con el propósito de evitar probabilidades iguales a cero. Finalmente, se divide a este vector de puntajes por la suma de todos sus componentes, para convertirlo en probabilidades. Este resultado final se guarda en la fila correspondiente de la matriz de probabilidades. Luego de iterar por todas las clases, el modelo queda entrenado y está apto para realizar predicciones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-draft",
   "metadata": {},
   "source": [
    "#### 2.2. Predicción\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "    Para predicir, primero se tomarán los textos a clasificar y se guardarán las repeticiones de cada palabra en cada noticia en una matriz esparza. Cabe destacar que sólo serán tenidas en cuenta las palabras con las que el modelo fue entrenado. Con el proposito de utilizar \"Naive Bayes\", se quiere aplicar la siguiente fórmula:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-stocks",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $P(x_1,x_2,...,x_N|y) = \\prod_{i=1}^N P(x_i|y)^{n_{ij}}$\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-limit",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "    Donde $x_i$ es cada una de la palabras del vocabulario y $n_{ij}$ la cantidad de repeticiones de la palabra $x_i$ en el texto \"j\". Esta probabilidad obtenida será el likelihood naive, asumiendo independencia y approach BOW. Luego, se calcula este likelihood para cada una de las clases \"y\", y a cada una se la multiplica por su probabilidad a priori obtenida durante el entrenamiento. Finalmente se dirá que el texto pertenecerá a la clase con mayor probabilidad a posteriori, o sea, $P(y|x_1,x_2,...,x_N)$, que será proporcional a $P(x_1,x_2,...,x_N|y)\\cdot P(y)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-singles",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "El único cambio que en este trabajo se le aplicó a la fórmula de Naive Bayes fue el hecho de comparar los logaritmos de $P(y|x)$. Esto se decidió hacer así para evitar tener números muy pequeños debido a los productos de probabilidades pequeñas. Por lo tanto, la fórmula de Naive Bayes se reescribe de la siguiente forma:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-reference",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $\\log(P(y|x)) = \\log(P(y)) + \\sum_{i=1}^N n_{ij}\\cdot \\log(P(x_i|y))$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-mentor",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En la fórmula anterior se puede ver la necesidad del $\\textit{smoothing}$ ya que, de no haberlo, se estarían evaluando logaritmos en cero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-filling",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "En el caso particular de este trabajo, con el fin de aplicar la fórmula mencionada, se tomará a la matriz de repeticiones provista por CountVectorizer, y se la multiplicará matricialmente por la matriz de logaritmos de probabilidades, traspuesta. Este producto matricial dará como resultado otra matriz, cuyas columnas serán las distintas clases y sus filas serán los distintos textos a clasificar. En los pares fila-columna $(i-j)$ estarán los logaritmos de likelihood del texto $i$ para la clase $j$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-memorial",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Si a cada columna luego se le suma el logaritmo de la probabilidad a priori de la clase correspondiente, entonces la matriz se transformará en una matriz cuyos valores serán proporcionales a los logaritmos de las probabilidades a posteriori de cada noticia para cada clase. Finalmente, el vector de predicciones serán las columnas con probabilidad a posteriori máxima de cada fila.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "marked-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier:\n",
    "    def __init__(self, method = ('tf', 'cv'), **kwargs):\n",
    "        if method in [('tf', 'cv'), ('tf', 'tf')]:\n",
    "            self.tfidf = TfidfVectorizer(**kwargs, stop_words = 'english')\n",
    "        else:\n",
    "            self.cv = CountVectorizer(**kwargs, stop_words = 'english')\n",
    "        self.method = method\n",
    "\n",
    "        self.got_hist = False\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, train_set, targets, smoothing = 0):\n",
    "        \n",
    "        self.got_hist = False\n",
    "        \n",
    "        self.train_set, self.targets_train = train_set, targets\n",
    "\n",
    "        self.groups = np.unique(self.targets_train)\n",
    "        \n",
    "        #              Laplacian Smoothing             #\n",
    "        #self.fitted_train = self.tfidf.fit(self.train_set)\n",
    "        #for group in self.groups:\n",
    "        #    self.targets_train = np.append(self.targets_train, group)\n",
    "        #    self.train_set.append(' '.join(smoothing * self.tfidf.get_feature_names()))\n",
    "        #\n",
    "        #self.fitted_train = self.tfidf.transform(self.train_set)\n",
    "        \n",
    "        if self.method in [('tf', 'tf'), ('tf', 'cv')]:\n",
    "            self.fitted_train = self.tfidf.fit_transform(self.train_set)\n",
    "            if self.method == ('tf', 'cv'):\n",
    "                self.cv = CountVectorizer(stop_words = 'english', vocabulary = self.tfidf.get_feature_names())\n",
    "        else:\n",
    "            self.fitted_train = self.cv.fit_transform(self.train_set)\n",
    "            if self.method == ('cv', 'tf'):\n",
    "                self.tfidf = TfidfVectorizer(stop_words = 'english', use_idf = True, sublinear_tf = True, norm = 'l2', vocabulary = self.cv.get_feature_names())\n",
    "                self.tfidf.fit(self.train_set)\n",
    "            \n",
    "        self.priori = np.mean(self.targets_train == self.groups.reshape(-1, 1), axis = 1)\n",
    "        \n",
    "        self.probabilities = np.zeros((20, self.fitted_train.shape[1]))\n",
    "    \n",
    "        for i, group in enumerate(self.groups):\n",
    "            row = self.fitted_train[self.targets_train == group].sum(axis = 0)\n",
    "            # Smoothing\n",
    "            row += row[row > 0].min()\n",
    "            self.probabilities[i] = row / row.sum()\n",
    "        \n",
    "        self.fitted = True\n",
    "            \n",
    "    def df_hist(self, **kwargs):\n",
    "        if not self.got_hist:\n",
    "            df = self.fitted_train.getnnz(axis = 0) / self.targets_train.size\n",
    "        \n",
    "            self.hist_height, self.hist_x, container = plt.hist(df * 100, bins = int(np.sqrt(df.size)), **kwargs)\n",
    "            self.hist_width = (self.hist_x[-1] - self.hist_x[0]) / (self.hist_x.size - 1)\n",
    "            self.hist_x = (self.hist_x[:-1] + self.hist_x[1:])/2\n",
    "            self.got_hist = True\n",
    "        else:\n",
    "            plt.bar(self.hist_x, self.hist_height, width = self.hist_width)\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('%')\n",
    "        if 'density' in kwargs and kwargs['density']: plt.ylabel('Density')\n",
    "        else: plt.ylabel('Word count')\n",
    "            \n",
    "    def score(self, test_set, targets):\n",
    "        return np.mean(self.predict(test_set) == targets)\n",
    "    \n",
    "    def check_fitted(self):\n",
    "        if not self.fitted: raise Exception ('Cannot predict before fitting.')\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        self.check_fitted()\n",
    "        \n",
    "        if self.method in [('tf', 'cv'), ('cv', 'cv')]: fitted_test = self.cv.transform(input_data)\n",
    "        else: fitted_test = self.tfidf.transform(input_data)\n",
    "            \n",
    "        probs_test = fitted_test.dot(np.log(self.probabilities).T) + np.log(self.priori)\n",
    "        \n",
    "        return self.groups[np.argmax(probs_test, axis = 1)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-papua",
   "metadata": {},
   "source": [
    "### 3. Análisis Exploratorio de Datos\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Para ver cómo se distribuyen las frecuencias entre documentos, se entrenó al modelo con los datos de entrenamiento, y se llamó a la función \"df_hist\". Ésta plotea un histograma de las frecuencias entre documentos. En la figura se puede ver que prácticamente no hay palabras que aparezcan en más del 50% de los documentos. Este dato se utilizará como referencia para elegir el hiperparámentro max_df. Además se puede ver que la gran mayoría de las palabras aparecen en un bajo porcentaje de los documentos. Este dato se utilizará para la elección del hiperparámetro min_df.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stretch-helmet",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = 1.0)                    \n",
    "classifier.fit(train_data, targets_train)\n",
    "classifier.df_hist(density = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-validity",
   "metadata": {},
   "source": [
    "### 4. Selección de hiperparámetros\n",
    "\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Para la elección de los hiperparámetros se utilizaron los datos de validación. Se iteró por todas las combinaciones de hiperparámetros con el fin de encontrar la combinación que maximice la métrica elegida para este ejercicio, que es el $\\textit{accuracy}$.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "historical-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_df: 0.56\n",
      "Best smoothing: False\n",
      "Best use_idf: True\n",
      "Best subtf: True\n",
      "Best norm: l2\n",
      "Best result: 0.8908528501988511\n"
     ]
    }
   ],
   "source": [
    "max_df = np.arange(.01, .57, .05)\n",
    "smooth_idf = np.array([True, False])\n",
    "use_idf = np.array([True, False])\n",
    "sublinear_tf = np.array([True, False])\n",
    "norm = np.array(['l1', 'l2'])\n",
    "#validation, test, target_v, target_t = train_test_split(test_data, targets_test, test_size = 0.5, random_state = 0)\n",
    "\n",
    "res = np.zeros((max_df.size, smooth_idf.size, use_idf.size, sublinear_tf.size, norm.size))\n",
    "\n",
    "for i, df in enumerate(max_df):\n",
    "    for j, smooth in enumerate(smooth_idf):\n",
    "        for z, idf in enumerate(use_idf):\n",
    "            for k, subtf in enumerate(sublinear_tf):\n",
    "                for l, nrm in enumerate(norm):\n",
    "                    classifier = NewsClassifier(max_df = df, smooth_idf = smooth, use_idf = idf, sublinear_tf = subtf, norm = nrm, method = ('tf', 'tf'))\n",
    "                    \n",
    "                    classifier.fit(train_data, targets_train)\n",
    "                    res[i, j, z, k, l] = classifier.score(val_data, targets_val)\n",
    "                    \n",
    "best = res.max()\n",
    "argmax = np.argwhere(res == best)[-1]\n",
    "print(f'Best max_df: {max_df[argmax[0]]}')\n",
    "print(f'Best smoothing: {smooth_idf[argmax[1]]}')\n",
    "print(f'Best use_idf: {use_idf[argmax[2]]}')\n",
    "print(f'Best subtf: {sublinear_tf[argmax[3]]}')\n",
    "print(f'Best norm: {norm[argmax[4]]}')\n",
    "print(f'Best result: {best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cf159-97da-4957-9a20-a424da7550bb",
   "metadata": {},
   "source": [
    "### 5. Comparación entre combinaciones de TfidfVectorizer y CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b29dd-6abd-4032-b049-ec58dfe8e38a",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Habiendo encontrado los hiperparámetros que mejor ajustaban a los datos de validación, se prosiguió a verificar dicho resultado para distintas combinaciones entre modelo de entrenamiento y predicción ($\\textit{CountVectorizer}$ y $\\textit{TfidfVectorizer}$).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2d436b2-f79c-493c-863b-191cc6df4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = lambda x: str(np.round(x * 100, 3)) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07194dd-09b7-4da2-9c68-827a2b89171c",
   "metadata": {},
   "source": [
    "#### 5.1 Fitting: CountVectorizer, Prediction: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75962761-6294-4de1-aae0-90a2d3eb6089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 89.371%\n",
      "Valid: 80.734%\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = .56, method = ('cv', 'cv'))\n",
    "classifier.fit(train_data, targets_train)\n",
    "print('Train:', percent(classifier.score(train_data, targets_train)))\n",
    "print('Valid:', percent(classifier.score(val_data, targets_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cec07-0241-4169-a935-94e5b873e8c0",
   "metadata": {},
   "source": [
    "#### 5.2 Fitting: TfidfVectorizer, Prediction: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1138bb7c-a0e4-4b4a-93d9-d3abf292a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 98.829%\n",
      "Valid: 89.085%\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = .56, method = ('tf', 'tf'), use_idf = True, sublinear_tf = True, norm = 'l2')\n",
    "classifier.fit(train_data, targets_train)\n",
    "print('Train:', percent(classifier.score(train_data, targets_train)))\n",
    "print('Valid:', percent(classifier.score(val_data, targets_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5459a2b-5dd6-496e-8c19-93f0903d794e",
   "metadata": {},
   "source": [
    "#### 5.3 Fitting: TfidfVectorizer, Prediction: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d293118-07cd-4229-8ea7-475596699f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 98.299%\n",
      "Valid: 87.981%\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = 0.46, method = ('tf', 'cv'), sublinear_tf = True, use_idf = True, norm = 'l2')\n",
    "classifier.fit(train_data, targets_train)\n",
    "print('Train:', percent(classifier.score(train_data, targets_train)))\n",
    "print('Valid:', percent(classifier.score(val_data, targets_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be3544-3421-42b8-9096-9f7300c57bda",
   "metadata": {},
   "source": [
    "#### 5.4 Fitting: CountVectorizer, Prediction: TfidftVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fc9b760-4466-48de-9e0a-1ac026539280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 92.299%\n",
      "Valid: 83.164%\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = 0.56, method = ('cv', 'tf'))\n",
    "classifier.fit(train_data, targets_train)\n",
    "print('Train:', percent(classifier.score(train_data, targets_train)))\n",
    "print('Valid:', percent(classifier.score(val_data, targets_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d361e1-2350-42c6-b418-66c84a42b038",
   "metadata": {},
   "source": [
    "Como se puede observar de los resultados anteriores, la mejor combinación para los datos de validación es hacer tanto el entrenamiento como la predicción con $\\textit{TfidfVectorizer}$. Por lo tanto, se tomó éste como modelo final, obteniendo la siguiente métrica:\n",
    "\n",
    "<center> $Accuracy = 89.085\\%$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-divide",
   "metadata": {},
   "source": [
    "#### 4.2 Testeo\n",
    "<div style=\"text-align:justify; text-indent: 25px\">\n",
    "Finalmente, se midió la métrica con los datos de test para el modelo encontrado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "progressive-production",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 79.235%\n"
     ]
    }
   ],
   "source": [
    "classifier = NewsClassifier(max_df = .46, method = ('tf', 'tf'), use_idf = True, sublinear_tf = True, norm = 'l2')\n",
    "classifier.fit(train_data, targets_train)\n",
    "print(f'Accuracy test: {percent(classifier.score(test_data, targets_test))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
